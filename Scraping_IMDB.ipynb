{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(120000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 120 seconds\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "%autosave 120\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape IMDB search of movies 'based on novels'\n",
    "I searched for most popular \"based-on-novel\" feature films,   \n",
    "released 1920-01-01 to 2017-04-01, with 1-9999999 votes,   \n",
    "user ratings between 1.0 - 10.0, running time of 1 - 999999 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imdb_index(a, b):\n",
    "    website = 'http://www.imdb.com/search/title?boxoffice_gross_us=1,9999999999&keywords=based-on-novel&release_date=1955,2017-04-01&title_type=feature&page=%d&ref_=adv_nxt'\n",
    "    for num in range(a, b):\n",
    "        url = website % num\n",
    "        soup = BeautifulSoup(requests.get(url).text, \"lxml\")\n",
    "        pretty_imdb = str(soup.prettify)\n",
    "        pickle.dump(pretty_imdb, open(\"scrape_%s.p\" % num, \"wb\"))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imdb_index(1, 201)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'Search' function: From files, scrape IMDBID, Ranks, Titles, Links, & Release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#when everything on the page works.\n",
    "\n",
    "def one_frame(n):\n",
    "    number = int(n)\n",
    "    soup = BeautifulSoup(pickle.load(open(\"scrape_%d.p\" %number, \"rb\")), \"lxml\")\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    ranks = []\n",
    "    title = []\n",
    "    links = []\n",
    "    imdbid = []\n",
    "    release = []\n",
    "\n",
    "    for num in range(0, 50):\n",
    "        item1 = str(soup.find_all('span', {'class' : 'lister-item-index unbold text-primary'})[num].text)\n",
    "        ranks.append(item1)\n",
    "    \n",
    "        item2 = str(soup.find_all('span', {'class' : 'lister-item-year text-muted unbold'})[num].text)\n",
    "        release.append(item2)\n",
    "\n",
    "        item3 = soup.find_all('span', {'class' : 'lister-item-index unbold text-primary'})[num].parent()[1]['href']\n",
    "        links.append(item3)\n",
    "    \n",
    "        item4 = str(soup.find_all('span', {'class' : 'lister-item-index unbold text-primary'})[num].parent()[1].text)\n",
    "        title.append(item4)\n",
    "    \n",
    "        item8 = soup.find_all('span', {'class' : 'userRatingValue'})[num].get('data-tconst')\n",
    "        imdbid.append(item8)\n",
    "    \n",
    "    del soup\n",
    "\n",
    "    df['ranks'] = ranks\n",
    "    df['title'] = title\n",
    "    df['imdbid'] = imdbid\n",
    "    df['links'] = links\n",
    "    df['release'] = release\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb_search = [] \n",
    "for number in range (1, 201):\n",
    "    data_imdb = one_frame(int(number))\n",
    "    imdb_search.append(data_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "super_imdb = pd.concat(imdb_search)\n",
    "# pickle.dump(super_imdb, open(\"super_imdb.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'Metascore' function: From files, scrape IMDBID & Metascrore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mscore_frame(a, b):\n",
    "    \n",
    "    metacritic_list = [] # empty list\n",
    "    for number in range(a, b):\n",
    "        soup = BeautifulSoup(pickle.load(open(\"scrape_%d.p\" % number, \"rb\")), \"lxml\")\n",
    "\n",
    "        df_mscore = pd.DataFrame()\n",
    "        imdbid = []\n",
    "        metascore = []\n",
    "\n",
    "        ratings = soup.find_all('div', {'class' : \"inline-block ratings-metascore\"})\n",
    "        length = len(ratings)\n",
    "\n",
    "        for num in range(0, length):\n",
    "            item12 = str(soup.find_all('div', {'class' : \"inline-block ratings-metascore\"})[num].parent()[-1].text)\n",
    "            metascore.append(item12)\n",
    "    \n",
    "            item13 = soup.find_all('div', {'class' : \"inline-block ratings-metascore\"})[num].parent()[4].get('data-tconst')\n",
    "            imdbid.append(item13)\n",
    "\n",
    "        df_mscore['imdbid'] = imdbid\n",
    "        df_mscore['metascore'] = metascore\n",
    "        metacritic_list.append(df_mscore)\n",
    "        \n",
    "        del soup\n",
    "\n",
    "    return metacritic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_score = mscore_frame(1, 201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "super_meta = pd.concat(total_score)\n",
    "# pickle.dump(super_meta, open(\"super_meta.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Merge 'Search' df & 'Metascore' df, & clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# super_imdb = pickle.load(open(\"super_imdb.p\", \"rb\"))\n",
    "# super_meta = pickle.load(open(\"super_meta.p\", \"rb\"))\n",
    "lean_results = super_meta.merge(super_imdb, on='imdbid', how='left')\n",
    "# pickle.dump(lean_results, open(\"lean_results.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'AmazonID' Function: Using IMDBID, scrape AmazonID & Amazon link from IMDB literature pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def amazon_frame(data, n):\n",
    "    \n",
    "    amz_df = pd.DataFrame()\n",
    "    imdbid = []\n",
    "    amazon = []\n",
    "    amazonid = []\n",
    "\n",
    "    website = 'http://www.imdb.com/title/%s/literature?ref_=tt_ql_dt_8'\n",
    "    for ttid in data[int(n):int(n+1)]['imdbid']:\n",
    "        url = website % ttid\n",
    "        soup = BeautifulSoup(requests.get(url).text, \"lxml\")\n",
    "        \n",
    "        try: \n",
    "            item15 = soup.select('a[href^=\"http://www.amazon.com/exec/\"]')[0]\n",
    "            amazon.append(item15)\n",
    "    \n",
    "            item16 = item15.text\n",
    "            amazonid.append(item16)\n",
    "    \n",
    "        except:\n",
    "            amazon.append(np.nan)\n",
    "            amazonid.append(np.nan)\n",
    "        \n",
    "        imdbid.append(ttid)    \n",
    "        del soup\n",
    "\n",
    "    amz_df['imdbid'] = imdbid\n",
    "    amz_df['amazon'] = amazon\n",
    "    amz_df['amazonid'] = amazonid\n",
    "    return amz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amazon_list = [] # empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page = 1\n",
    "for number in range (1, 1232):\n",
    "    print('Scraping page %d out of 1231' % page)\n",
    "    page +=1\n",
    "    amz_imdb = amazon_frame(lean_results, int(number))\n",
    "    amazon_list.append(amz_imdb)\n",
    "# function breaks when page has a different layout. skip & continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_amazon = pd.concat(amazon_list)\n",
    "final_amazon = data_amazon.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Merge 'AmazonID' df with 'Search+Metascore' df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "almost_final = final_amazon.merge(lean_results, on='imdbid', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'Gross' function: Using IMDBID, scrape Gross Box Office from IMDB business pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gross_frame(data, n):\n",
    "\n",
    "    money_df = pd.DataFrame()\n",
    "    imdbid = []\n",
    "    gross = []\n",
    "    \n",
    "    website = 'http://www.imdb.com/title/%s/business?ref_=tt_dt_bus'\n",
    "    for ttid in data[int(n):int(n+1)]['imdbid']:\n",
    "        url = website % ttid\n",
    "        soup = BeautifulSoup(requests.get(url).text, \"lxml\")\n",
    "        \n",
    "        try: \n",
    "            parent = soup.find_all('div', {'id' : 'tn15content'})[0]\n",
    "            for child in parent.find_all('h5'):\n",
    "                if(\"Gross\" in child):\n",
    "                    gross.append(str(child.next_sibling.strip()))\n",
    "        except:\n",
    "            gross.append(np.nan)\n",
    " \n",
    "        imdbid.append(ttid)\n",
    "        del soup\n",
    "\n",
    "    money_df['imdbid'] = imdbid\n",
    "    money_df['gross'] = gross \n",
    "    return money_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gross_list = [] # empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 645 out of 645\n"
     ]
    }
   ],
   "source": [
    "page = 1\n",
    "for number in range (1, 646):\n",
    "    print('Scraping page %d out of 645' % page)\n",
    "    page +=1\n",
    "    df = gross_frame(almost_final, int(number))\n",
    "    gross_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gross_df = pd.concat(gross_list).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Merge 'Gross' df with 'Search+Metascore+AmazonID' df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_imdb = gross_df.merge(almost_final, on='imdbid', how='left')\n",
    "final_imdb = final_imdb.dropna()\n",
    "# final_imdb.to_pickle('final_imdb.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_cleaner(feature):\n",
    "    empty_list = []\n",
    "    for s in final_imdb[feature]:\n",
    "        s = re.sub(\"[^0-9]\", \"\", s)\n",
    "        empty_list.append(s)\n",
    "    return empty_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Box Office & Metascore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# final_imdb = pickle.load(open(\"final_imdb.p\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_imdb['box_office'] = feature_cleaner('gross')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_imdb['box_office'] = final_imdb['box_office'].astype(np.int64)\n",
    "final_imdb['metascore'] = final_imdb['metascore'].astype(np.int64)\n",
    "final_imdb['amazon'] = final_imdb['amazon'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_imdb['release'] = feature_cleaner('release')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(final_imdb, open(\"final_imdb.p\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "105px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
